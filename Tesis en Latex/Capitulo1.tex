\chapter{Fundamentaci\'{o}n te\'{o}rica}

  Los bancos cumplen importantes funciones en la sociedad mediante principales funciones que son\cite{1}:
  
  \begin{itemize}
  	\item Canalizaci\'{o}n del ahorro a trav\'{e}s de la demanda de una rentabilidad por la confianza del cliente de su dep\'{o}sito de capital en el banco.
  	\item Seguridad en el dep\'{o}sito de capital. Los bancos guardan el dinero de las personas y tienen sistemas de seguridad muy potentes que permiten garantizar el dinero de sus clientes.
  	\item Pr\'{e}stamos y cr\'{e}dito. Puede ser con dis\'{i}miles motivos, tanto para la inversi\'{o}n de un negocio o la adquisici\'{o}n de bienes o maquinarias.
  	\item Productos financieros.
  	\item Control del dinero en circulaci\'{o}n.
  	\item Cumplimiento de las ratios m\'{i}nimas de reservas para garantizar la liquidez de la masa de capital de sus clientes.
  	\item Equilibrar el cociente entre expansi\'{o}n del cr\'{e}dito y volumen de dep\'{o}sitos.
  	\item Ofrecer servicios de asesoramiento financiero y patrimonial.
  	\item Permite aplazar pagos y uso de tarjetas de cr\'{e}dito y de d\'{e}bito.
  \end{itemize}

  Las tareas relacionadas con el banco que son realizadas por algoritmos de \textit{Machine Learning} son: La detecci\'{o}n y prevenci\'{o}n de fraude, poner precios a las acciones, la gesti\'{o}n de riesgo, la gesti\'{o}n de crisis, la atribuci\'{o}n del cliente, la deserci\'{o}n de clientes, los bancos minoristas, la retenci\'{o}n de clientes, la aprobaci\'{o}n de cr\'{e}ditos y el m\'{a}rketing . El uso de la inteligencia artificial en las actividades bancarias mejora la seguridad de los datos y capitales de los clientes como la salud de las instituciones financieras. Estos se agrupan en cinco grandes grupos como: la mejora de la experiencia del cliente, los \textit{chatbots}, las ofertas personalizadas, la retenci\'{o}n de clientes y la detecci\'{o}n de fraude bancario.
  
  La detecci\'{o}n de fraudes y lavado de dinero cumplen una tarea fundamental de los bancos, que le infundan una gran importancia dentro de la sociedad en que se encuentra. Mediante la inteligencia artificial, esta y las dem\'{a}s tareas de los bancos son optimizadas y mejoradas, entre los beneficios que aporta se encuentran\cite{3}:
  
  \begin{itemize}
  	\item Mayor automatizaci\'{o}n y mejoramiento de la productividad, permite a los empleados evitar realizar actividades repetitivas que consumen una gran cantidad de tiempo, poder dedicar tiempo a papeleos y la atenci\'{o}n a los clientes.
  	\item Servicio personalizado al cliente a un menor costo, usando la capacidad de \textit{Big Data} se puede rastrear y almacenar las operaciones y tendencias del cliente para de esta forma crear y ayudar al cliente de una manera \'{o}ptima seg\'{u}n sus necesidades.
  	\item Mayor precisi\'{o}n de riesgo de activos, teniendo un gran c\'{u}mulo de informaci\'{o}n del cliente evita una posible suplantaci\'{o}n de identidad.
  	\item Avanzada detecci\'{o}n y prevenci\'{o}n de fraude, este es el m\'{a}ximo beneficio de la inteligencia artificial para cualquier instituci\'{o}n financiera por la hist\'{o}rica costumbre de los criminales de cometer ilegalidades financieras.
  \end{itemize}

  En el sistema bancario actual una significativa mayor\'{i} de las transacciones es realizadas mediante tarjetas de cr\'{e}dito o transacciones electr\'{o}nicas bancarias. Esto permite tener un gran c\'{u}mulo de informaci\'{o}n digital para la detecci\'{o}n de fraudes bancarios y para ello existen varias herramientas como la t\'{e}cnica de detecci\'{o}n de anomal\'{i}as. Esta es una t\'{e}cnica de ML que detecta los fraudes autom\'{a}ticamente casi en tiempo real mediante la identificaci\'{o}n de correlaciones ocultas en la informaci\'{o}n.
  
  La detecci\'{o}n de anomal\'{i}as mediante ML permite a trav\'{e}s de un conjunto de datos como entrada, obtener reglas. Para la detecci\'{o}n de anomal\'{i}as el conjunto de datos para entrenamiento se clasifica en:
  
  \begin{itemize}
  	\item Supervisado: se provee un conjunto de datos con una elevada cantidad de muestras an\'{o}malas o normales.
  	\item Semi-supervisado: se provee un conjunto de datos con todas las muestras an\'{o}malas o normales, no existe la combinaci\'{o}n.
  	\item No supervisados: no se provee conjunto de datos.
  \end{itemize}

\begin{figure}
	\centering
	\includegraphics[width=1.1\textwidth]{"figuras/Fig1"}
	\caption{Clasificaci\'{o}n de Machine Learning}
\end{figure}

  \textit{Machine Learning} est\'{a} presente en una gran \'{a}rea del procesamiento de datos, incluido la identificaci\'{o}n de fraude de tarjeta de cr\'{e}dito. La detecci\'{o}n del fraude de tarjeta se centra en las acciones de transferencia realizadas en la misma. Para ellos es necesario un conjunto de datos para el entrenamiento que posea una mezcla de los dos tipos de transacciones, los fraudes y los normales. Por ende, se debe usar un aprendizaje supervisado, para ello existen diferentes t\'{e}cnicas como son\cite{4}:
  
  \begin{itemize}
  	\item \textit{\textbf{Random Forest (RF)}}: es una metodolog\'{i}a usada solo para mejorar su prosperidad y precisi\'{o}n en algoritmos de ML, tambi\'{e}n puede ayudar a identificar las variables independientes adecuadas para la selecci\'{o}n de funcionalidad del sistema.
  	\item \textit{\textbf{Naive Baiyes Classifier (NBC)}}: es un proceso estad\'{i}stico basado en teor\'{i}a predictiva que selecciona su mejor decisi\'{o}n seg\'{u}n la probabilidad, tambi\'{e}n permite la implementaci\'{o}n de conocimientos previos como l\'{o}gica en afirmaciones impredecibles.
  	\item \textit{\textbf{Logistic Regression (LR)}}: Es otro m\'{e}todo decidido a tomar prestado de la profesi\'{o}n de datos est\'{e}ticos por ML. Es tambi\'{e}n el proceso de referencia de asuntos concernientes a la categorizaci\'{o}n binaria. En los casos de detecci\'{o}n de amenazas de fraude con tarjetas de cr\'{e}dito, se usa la clase de distribuci\'{o}n de probabilidad como fraudulenta y no como fraude.
  	\item \textit{\textbf{Support Vector Machine (SVM)}}: Es un modelo supervisado, usa algoritmos de clasificaci\'{o}n de aprendizaje para clasificar problemas importantes incluso en dos grupos e individuales. Pueden clasificar un nuevo documento partiendo de un n\'{u}mero de datos nombrados a cada clasificaci\'{o}n en un sistema SVM.
  	\item \textit{\textbf{K-Nearest Neighbors (KNN)}}: es un clasificador simple de implementar algoritmo supervisado de ML que puede ser usado para direccionar la clasificaci\'{o}n respectiva como las dificultadas regresiones.
  	\item \textit{\textbf{Classification Trees}}: El \'{a}rbol de clasificaci\'{o}n marca, graba y asigna factores de clases separadas, es construido por un proceso llamado particionado recursivo binario.
  	\item \textit{\textbf{Artificial Neural Network (ANN)}}: Este es un m\'{e}todo de ML basado en el sistema nervioso del cerebro. Mediante sus dise\~{n}os y utilizando informaci\'{o}n hist\'{o}rica, pueden descubrir tendencias y clasificar la informaci\'{o}n entrante.
  	\item \textit{\textbf{Restricted Boltzman Machines (RBM)}}: es un algoritmo especializado en la detecci\'{o}n de fraudes. El algoritmo aprende de la probabilidad distribuida de los datos de entrada en un modelo no supervisado, no posee una capa neuronal de salida, solo una capa visible y otra oculta. Su importancia es que intenta identificar con que probabilidad un objeto particular podr\'{i}a activar una caracter\'{i}stica particular \cite{5}.
  	\item \textit{\textbf{Gradient Boosting (GBM)}}: Es un algoritmo prominente de ML, siempre tiene que realizar la clasificaci\'{o}n como actividades de regresi\'{o}n. El modelo consiste en una cantidad fundamental de dise\~{n}os ensamblados como un \'{a}rbol de decisi\'{o}n d\'{e}bil.
  	\item \textit{\textbf{Isolation Forest (IF)}}: es especialmente dise\~{n}ado para la detecci\'{o}n de anomal\'{i}as sin importar el tama\~{n}o del conjunto de datos, consiste en la creaci\'{o}n de varios \'{a}rboles donde se van particionado al mismo tiempo que se van clasificando \cite{6}.
  	\item \textit{\textbf{Local Outlier Factor (LOF)}}: Este algoritmo fue hecho especialmente para la detecci\'{o}n de anomal\'{i}as basados en otros como DBSCAN\footnote{The density-based spatial clustering application with noise (DBSCAN)}  o KNN. Este algoritmo crea una K cantidad de vecindarios y mediante sus f\'{o}rmulas calcula la distancia m\'{a}xima que debe estar un valor de su vecino, si es mayor se considera una anomal\'{i}a.
  \end{itemize}

\begin{figure}[h!]
	\centering
	\includegraphics[height=0.9\textheight]{"figuras/Fig2"}
	\caption{Campos de la miner\'{i}a de datos}
\end{figure}

  Unos de los mayores retos de la identificaci\'{o}n de fraude con tarjeta de cr\'{e}dito es la enorme informaci\'{o}n que se almacenan de las operaciones de las tarjetas, adem\'{a}s de que a visi\'{o}n general existe el problema de la informaci\'{o}n desbalanceada. La clasificaci\'{o}n desbalanceada\footnote{Es un problema de modelado predictivo de clasificaci\'{o}n donde la distribuci\'{o}n de ejemplos de las clases no es igual.} tiene dos principales causas que son las muestras de datos y las propiedades del dominio \cite{7}, en este caso se refiere a la primera causa, donde la mayoría de las transacciones, cerca de un 99\% no son fraude, siendo casi imposible detectar actividades fraudulentas. Existen t\'{e}cnicas para aliviar el problema del desbalance entre las clases de informaci\'{o}n.
  
  Una de las t\'{e}cnicas para la soluci\'{o}n de problemas desbalanceados es el sub muestreo (\textit{Undersample} en ingl\'{e}s), esta t\'{e}cnica consiste en la selecci\'{o}n aleatoria de ejemplos de la clase mayoritaria y su eliminaci\'{o}n del conjunto de datos de entrenamiento. Una limitaci\'{o}n de esta t\'{e}cnica es que los ejemplos se eliminan sin preocuparse por su decisi\'{o}n entre las clases, por lo tanto, es posible que se elimine informaci\'{o}n \'{u}til \cite{8}.
  
  Otra forma de resolver este problema es el sobre muestreo (\textit{Oversample}) de los ejemplos en la clase minoritaria, puede ser duplicando ejemplos de la clase minoritaria en el conjunto de datos de entrenamiento antes de ajustar un modelo o sintetizando nuevos ejemplos de la clase minoritaria \cite{9}. El enfoque más utilizado para sintetizar nuevos ejemplos es \textit{Synthetic Minority Oversampling TEchnique} (SMOTE), funciona seleccionando ejemplos cercanos en el espacio de caracter\'{i}sticas, dibujando una l\'{i}nea entre los ejemplos en el espacio de caracter\'{i}sticas y dibujando una nueva muestra en un punto a lo largo de esta l\'{i}nea \cite{10}. Tambi\'{e}n aparece el \textit{Adaptive Synthetic} (ADASYN) que es un algoritmo que genera datos sint\'{e}ticos y su mayor ventaja es no copiar los mismos datos minoritarios \cite{11}.
  
  En la pasada d\'{e}cada se han realizado varias investigaciones sobre posibles soluciones acerca del problema desbalanceado tomando de base al algoritmo KNN. Estos grupos de m\'{e}todos est\'{a}n basados en \cite{12}:
  
  \begin{itemize}
  	\item Estrategias de peso, estos m\'{e}todos asignan pesos a las muestras de entrenamiento en el vecindario de una muestra testeada.
  	\item Estructura geom\'{e}trica local de datos.
  	\item L\'{o}gica difusa, la pertenencia de cada clase es asignada a un ejemplo como una etiqueta de clase n\'{i}tida, que puede preservar abundante informaci\'{o}n clasificada y entonces crear una clasificaci\'{o}n completa.
  	\item La falta de estimaci\'{o}n de datos positivos.
  	\item M\'{e}trica de distancia novedosa.
  	\item El tama\~{n}o din\'{a}mico de los vecindarios.
  \end{itemize}

  \textit{Big Data} es el conjunto inmenso y diverso de informaci\'{o}n que se incrementa constantemente, conforma tambi\'{e}n el volumen de informaci\'{o}n, la velocidad con que es creada o recolectada la misma. Puede ser categorizada en no estructurada o estructurada: los datos estructurados consisten en informaci\'{o}n administrada por la organizaci\'{o}n en base de datos u hojas de c\'{a}lculo, mientras que los datos no estructurados es la informaci\'{o}n desorganizada que no se delimita por un formato o modelo \cite{13}. Las fuentes de datos son las redes sociales, motores de b\'{u}squedas de internet, plataformas de comercio electr\'{o}nico, cines en l\'{i}nea, entre otras formas de interacci\'{o}n en l\'{i}nea. Normalmente la informaci\'{o}n es no estructurada, es tan vasta que tomar\'{i}a demasiado tiempo por los humanos para extraer informaci\'{o}n relevante y \'{u}til.
  
  Los retos del \textit{Big Data} est\'{a}n ampliamente definidos en las cinco Vs: valor, veracidad, variedad, velocidad y volumen. Valor se refiere a los beneficios asociados con el an\'{a}lisis de datos, la veracidad a la precisi\'{o}n de los datos; variedad es la cantidad de tipos de datos, como los estructurados, semi-estructurados o los no estructurados. El volumen es la cantidad de datos que es acumulada y la velocidad se refiere con la gran rapidez con que se generan los datos y sus muchas dimensiones \cite{14}. Con mayor cantidad de dimensiones se crean dificultades para la detecci\'{o}n de anomal\'{i}as, porque cuando el n\'{u}mero de atributos incrementa, la cantidad de datos necesarios para categorizar aumenta, concluyendo en la dispersi\'{o}n de datos.
  
  Un subconjunto de t\'{e}cnicas de ML com\'{u}nmente usada para procesar \textit{Big Data} es \textit{Deep Learning} (DL), tambi\'{e}n conocido como \textit{Deep Neural Networks} o \textit{Neural Learning}, es un subconjunto de ML, utiliza niveles de jerarqu\'{i}a de ANN para llevar procesos de ML. Comparada con t\'{e}cnicas de ML como SVM y KNN, DL posee ventajas de los aprendizajes no supervisados, una fuerte capacidad de generalizaci\'{o}n, y un poderoso entrenamiento robusto para \textit{Big Data} \cite{15}. El DL posee grandes ventajas con respecto a ML, una de ellas es la redundancia de la extracci\'{o}n de caracter\'{i}sticas.
  
  Los m\'{e}todos tradicionales de ML (\textit{Decision Tree} (DT), SVM y LR) eran los m\'{a}s populares, las extracciones de caracter\'{i}sticas suelen ser complicadas y requiere conocimiento detallado del dominio del problema. Este paso debe adaptarse, probarse y perfeccionarse en varias iteraciones para obtener resultados \'{o}ptimos.
  
  Los modelos de DL no necesitan la extracci\'{o}n de caracter\'{i}sticas, en estos casos se tienen ANN. Las capas pueden aprender una representaci\'{o}n impl\'{i}cita de los datos sin procesar por s\'{i} mismas. Un modelo de DL produce una representaci\'{o}n abstracta y comprimida de los datos sin procesar en varias capas de una ANN, luego son usadas para producir el resultado. Los modelos de DL requieren poco o ning\'{u}n esfuerzo manual para realizar y optimizar el proceso de extracci\'{o}n de caracter\'{i}sticas, es decir, est\'{a} integrada en el proceso que tiene lugar dentro de la ANN.
  
  Otra gran ventaja del DL es que funciona con cantidades masivas de datos. Los modelos de DL tienden a aumentar su precisi\'{o}n con la creciente cantidad de datos de entrenamiento, mientras que los modelos tradicionales de ML dejan de mejorar despu\'{e}s de un punto de saturaci\'{o}n.
  
  Se han creado varios algoritmos que permiten la detecci\'{o}n de fraudes bancarios, a continuaci\'{o}n, se desglosan algunos de los algoritmos:
  
  \begin{itemize}
  	\item \textit{\textbf{Convolutional Neural Network (CNN)}}: CNN es un m\'{e}todo DL altamente asociado con datos especiales, similar a ANN, posee la misma estructura de capa oculta en adici\'{o}n a la capa especial convolucional con diferente n\'{u}mero de canales en cada capa \cite{16}.
  	\item \textit{\textbf{Recurrent Neural Network (RNN)}}: RNN es un acercamiento din\'{a}mico de ML capaz de analizar los comportamientos temporales din\'{a}micos de varias cuentas bancarias por la modelaci\'{o}n de dependencias secuenciales entre transacciones consecutivas de los due\~{n}os de las tarjetas de cr\'{e}dito \cite{17}. En otras palabras, RNN es una red neuronal con memoria que tiende a tener un corto t\'{e}rmino de memoria por el problema de desaparici\'{o}n de gradiente. \textit{Backpropagation} es la columna vertebral de las redes neuronales, tanto que minimiza la p\'{e}rdida ajustando pesos de red que son encontrados usando gradientes.
  	\item \textit{\textbf{Back-Propagation Neural Network (BPNN)}}: BPNN es una multicapa \textit{feedforward neural network (FNN)}, es uno de los modelos ANN ampliamente usados. La regla de aprendizaje es usar el m\'{e}todo del descendiente m\'{a}s empinado y modificar repetidamente los pesos y bases de la red a trav\'{e}s de una iteraci\'{o}n reversa, entonces la suma de los cuadrados de los errores es minimizada \cite{18}.
  	\item \textit{\textbf{Long Short Term Memory Network (LSTM)}}: LSTM es un tipo de RNN mejorado, resuelve el problema del corto t\'{e}rmino de memoria. Tiene celdas de estado que es la memoria de la red, que existe por cada paso, adem\'{a}s de puertas en cada paso que controla el flujo de memoria que mantiene necesariamente y descarta informaci\'{o}n irrelevante \cite{16}.
  	\item \textit{\textbf{Autoencoder (AE)}}: AE es una clase especial de algoritmo DL, donde la salida es la misma que la entrada, pero tiene una capa medio u oculta que contiene menos neuronas y comprime los datos. AE usa varias capas para codificar y para decodificar en la capa oculta \cite{5}. Cuando los datos son decodificados, la salida es comparada con la entrada y si no son iguales, se activa un mecanismo de correcci\'{o}n hasta que se alcance el m\'{i}nimo error. En este proceso son detectadas las anomal\'{i}as, las cuales son en el caso de la problem\'{a}tica, las transacciones fraudulentas.
  	\item \textit{\textbf{Denoising Autoencoder (DAE)}}: DAE es una extensi\'{o}n de AE, que permite aprender m\'{a}s filtros robustos en las capas ocultas, reduce el riesgo de sobreajuste y previene de aprender una funci\'{o}n de identificaci\'{o}n simple \cite{19}.
  	\item \textit{\textbf{Dropout Regularization}}: \textit{Dropout} es una t\'{e}cnica donde aleatoriamente selecciona neuronas que son ignoradas durante el entrenamiento, esto significa que sus contribuciones a la activaci\'{o}n de neuronas v\'{i}a abajo es temporalmente removida en el paso hacia adelante y ning\'{u}n peso es actualizado a la neurona en el paso hacia atr\'{a}s \cite{20}.
  \end{itemize}

  Varios investigadores han creado sus propias versiones de algoritmos DL mediante el aumento de neuronas o creaci\'{o}n de h\'{i}bridos. Por lo tanto, existen m\'{e}tricas para demostrar la efectividad de estos modelos por la cantidad de aciertos \cite{21}. Estos son:
  
  \begin{itemize}
  	\item \textit{\textbf{Accuarcy}}: es el n\'{u}mero total de predicciones correctas dividido por el n\'{u}mero total de predicciones.
  	\item \textit{\textbf{Precision}}: define cuan confiable es un modelo en responder si un punto pertenece a una clase.
  	\item \textit{\textbf{Recall}}: expresa cuan bien puede el modelo detectar a una clase.
  	\item \textit{\textbf{F1 Score}}: es dada por la media harmon\'{i}a de \textit{precision} y \textit{recall}, combina ambas m\'{e}tricas en una sola.
  \end{itemize}

  Teniendo en cuenta estas m\'{e}tricas existen cuatro resultados posibles, las cuales se pueden interpretar como:
  
  \begin{itemize}
  	\item \textbf{Alta \textit{precision} y alto \textit{recall}}: el modelo maneja perfectamente la clase.
  	\item \textbf{Alta \textit{precision} y bajo \textit{recall}}: el modelo no detecta la clase muy bien, pero cuando lo hace es altamente confiable.
  	\item \textbf{Baja \textit{precision} y alto \textit{recall}}: la clase detecta bien la clase, pero tambi\'{e}n incluye muestras de otras clases.
  	\item \textbf{Baja \textit{precision} y bajo \textit{recall}}: El modelo no logra clasificar la clase correctamente.
  \end{itemize}

  Para el desarrollo y funcionamiento de los algoritmos en un sistema de c\'{o}mputo es necesario la utilizaci\'{o}n de tecnolog\'{i}as y herramientas de programaci\'{o}n. Una de las tecnolog\'{i}as a tener en cuenta son los lenguajes de programaci\'{o}n, este debe: poseer bastantes y buenas librer\'{i}as de ML y DL, debe tener un buen rendimiento en tiempo de ejecuci\'{o}n, un buen soporte de herramientas, una comunidad de programadores y un ecosistema saludable de paquetes de soporte. Algunos de los lenguajes que poseen estos requisitos son: Python\footnote{Python es un lenguaje de programaci\'{o}n interpretado cuya filosof\'{i}a hace hincapi\'{e} en la legibilidad del c\'{o}digo.}, C++\footnote{C++ es un lenguaje de programaci\'{o}n dise\~{n}ado para extender al lenguaje C y posee mecanismos que permiten la manipulaci\'{o}n de objetos.}, Java\footnote{Java es un lenguaje de programaci\'{o}n y una plataforma inform\'{a}tica.} y lenguajes de JVM\footnote{\textit{Java Virtual Machine} (JVM) es una m\'{a}quina virtual de proceso nativo, capaz de interpretar y ejecutar instrucciones expresadas en un c\'{o}digo binario especial.}, JavaScript\footnote{JavaScript es un lenguaje de programaci\'{o}n interpretado, dialecto del est\'{a}ndar ECMAScript.}, Swift\footnote{Swift es un lenguaje de programaci\'{o}n potente e intuitivo para iOS, iPadOS, macOS, tvOS y watchOS.} y lenguaje R\footnote{R es un entorno y lenguaje de programaci\'{o}n con un enfoque al an\'{a}lisis estad\'{i}stico.}.
  
  Para el desarrollo de este proyecto se hará uso del lenguaje de programaci\'{o}n Python en su versi\'{o}n 3.7.9, este lenguaje de programaci\'{o}n se encuentra a la cabeza de los m\'{a}s utilizados para DL, posee una alta gamma de librer\'{i}as, es un lenguaje sencillo y de alto rendimiento, posee bastantes comunidades de programadores que suben soluciones y repuesta sobre el tema. Este lenguaje lleva un destacado recorrido en el \'{a}rea del DL.

  Las librer\'{i}as que se usar\'{a}n en el proyecto son las siguientes:
  
  \begin{itemize}
  	\item Tensorflow.Keras: Keras es un API para personas que sigue buenas pr\'{a}cticas para reducir la carga cognitiva: ofrece consistencia y simple APIs, minimiza el n\'{u}mero requerido de acciones de usuario para casos de usos comunes \cite{22}.
  	\item Numpy: es una librer\'{i}a de Python usada para el trabajo con arreglos. Tambi\'{e}n tiene funciones para trabajar en el dominio de \'{a}lgebra linear y matrices \cite{23}.
  	\item Panda: es un paquete de Python que provee una estructura dise\~{n}ada de datos r\'{a}pida, flexible y expresiva para hacer trabajo con la clasificaci\'{o}n de datos \cite{24}.
  	\item Sickit-learn: es una herramienta de c\'{o}digo abierto, simple y eficiente para realizar an\'{a}lisis de datos predictivos \cite{25}.
  	\item Matplotlib: es una librer\'{i}a comprensiva para la agrupaci\'{o}n est\'{a}tica, animada y visualizaciones interactivas en Python \cite{26}.
  	\item Imbalanced-learn: proporciona herramientas para tratar el problema de informaci\'{o}n desbalanceada. En esta librer\'{i}a se encuentran los algoritmos de UnderSample, OverSample, SMOTE y ADASYN \cite{27}.
  \end{itemize}

  El IDE a utilizar es el PyCharm 2019.2.1 (\textit{Professional Edition})\cite{28}, es especializado para aplicaciones de Python.
  
  La miner\'{i}a de datos tambi\'{e}n conocida como \textit{Knowledge Discovery in Database} (KDD), es el proceso para descubrir patrones \'{u}tiles o conocimientos a partir de fuentes de datos. Los patrones deben ser v\'{a}lidos, potencialmente \'{u}tiles y entendibles. La miner\'{i}a de datos es un campo multidisciplinario que incluye: aprendizaje autom\'{a}tico, estad\'{i}sticas, sistemas de bases de datos, AI, \textit{Information Retrieval}, visualizaci\'{o}n de la informaci\'{o}n, etc. Entre las ventajas que ofrece se encuentran \cite{29}:
  
  \begin{itemize}
  	\item La miner\'{i}a de datos descubre informaci\'{o}n que no se esperaba obtener. Como muchos modelos diferentes son usados, algunos resultados inesperados tienden a aparecer. Las combinaciones de distintas t\'{e}cnicas otorgan efectos inesperados que se transforma en un valor a\~{n}adido a la empresa.
  	\item Enormes bases de datos pueden ser analizadas mediante la tecnolog\'{i}a de \textit{data mining}.
  	\item Los resultados son f\'{a}ciles de entender.
  	\item Contribuye a la toma de decisiones t\'{a}cticas y estrat\'{e}gicas para detectar la informaci\'{o}n clave.
  	\item Mejora la relaci\'{o}n con el cliente.
  	\item Los modelos son confiables. Los modelos son probados y comprobados usando t\'{e}cnicas estad\'{i}sticas antes de ser usado, para que las predicciones que se obtienen sean confiables y v\'{a}lidas.
  	\item En su mayor\'{i}a, los modelos se generan y construyen de manera r\'{a}pida. El modelado a veces se torna m\'{a}s f\'{a}cil puesto que muchos algoritmos han sido probados previamente. 
  \end{itemize}

  Existen metodolog\'{i}as para realizar la miner\'{i}a de datos, los cuales son \cite{30}:
  
  \begin{itemize}
  	\item \textit{Knowledge Discovery in Databases} (KDD): esta metodolog\'{i}a propone 5 fases: Selecci\'{o}n, pre procesamiento, transformaci\'{o}n, miner\'{i}a de datos y evaluaci\'{o}n e implementaci\'{o}n. Es un proceso iterativo e interactivo.
  	\item CRISP-DM: \textit{Cross-Industry Standard Process for Data Mining}, iniciativa financiada por la comunidad europea para desarrollar una plataforma para Miner\'{i}a de Datos.
  	\item SEMMA: es el acr\'{o}nimo a las 5 fases: \textit{Sample}, \textit{Explore}, \textit{Modify}, \textit{Model}, \textit{Assess}. La metodolog\'{i}a es propuesta por SAS Institute Inc.
  \end{itemize}

  Para este proyecto se aplicar\'{a} la metodolog\'{i}a KDD que es un an\'{a}lisis exploratorio, autom\'{a}tico y modelado de grandes repositorios de datos. Es un proceso organizado de identificaci\'{o}n v\'{a}lida, novel, \'{u}til, y entendibles patrones de un gran y complejo conjunto de datos \cite{31}. Para ello se definen los pasos de esta metodolog\'{i}a para orientar el desarrollo:
  
  \begin{enumerate}
  	\item Entendimiento, conocimientos previos e identificaci\'{o}n de la meta.
  	\item Selecci\'{o}n de un conjunto de datos.
  	\item Limpieza y preprocesamiento de datos.
  	\item Transformaci\'{o}n de los datos.
  	\item Colocar el objetivo del KDD a un m\'{e}todo de miner\'{i}a de datos.
  	\item Elecci\'{o}n de los algoritmos de miner\'{i}a de datos.
  	\item Implementaci\'{o}n de los algoritmos de miner\'{i}a de datos.
  	\item Evaluaci\'{o}n.
  	\item Aplicaci\'{o}n del conocimiento adquirido.
  \end{enumerate}

\begin{figure}[h!]
	\centering
	\includegraphics[height=0.5\textheight]{"figuras/Fig3"}
	\caption{Secuencia KDD}
\end{figure}

  En el pr\'{o}ximo cap\'{i}tulo se abarcar\'{a} la metodolog\'{i}a KDD secuencialmente atendiendo a los pasos definidos anteriormente.